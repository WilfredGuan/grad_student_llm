# graduate_student_LLM
## Initiatives
Hello there!

I'm currently a Master of Science in Computer Science (MSCS) student at Yale University, focusing my research on Large Language Models (LLMs). In the world of graduate studies, particularly in fields like ours, conducting expansive and resource-intensive experiments – such as training a model from the ground up – can be quite challenging due to resource constraints.

With this in mind, my project aims to explore areas like prompt tuning, instruction tuning, Parameter-Efficient Fine-Tuning (PEFT), Mixture of Experts (MoE), and Retrieval-Augmented Generation (RAG), among others. One interesting observation is that these methods often share similarities in their implementation pipelines and evaluation metrics. Additionally, the variety of open-source models readily available for these kinds of tasks is somewhat limited.

The goal of this project is to develop a comprehensive pipeline that facilitates smaller-scale experiments and enhances understanding of LLMs. This will include thorough documentation and well-structured, objective-oriented code modules, designed to be accessible and useful for fellow researchers and enthusiasts in the field.

I look forward to collaborating, learning, and advancing our collective knowledge in this exciting area of study!
